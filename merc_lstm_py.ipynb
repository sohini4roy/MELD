{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4IHkF3VOm/WE+ZA5/x3yi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohini4roy/MELD/blob/master/merc_lstm_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dBQUYEGHS-1"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "\n",
        "def set_seed(seed: int = 1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def lengths_to_mask(lengths: torch.Tensor, max_len: Optional[int] = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    lengths: (B,) long\n",
        "    returns mask: (B, T) with True for valid timesteps, False for padding\n",
        "    \"\"\"\n",
        "    B = lengths.size(0)\n",
        "    T = max_len if max_len is not None else int(lengths.max().item())\n",
        "    range_row = torch.arange(T, device=lengths.device).unsqueeze(0).expand(B, T)\n",
        "    mask = range_row < lengths.unsqueeze(1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "\n",
        "class JSONMERC(Dataset):\n",
        "    \"\"\"\n",
        "    Expects a JSON with:\n",
        "    [\n",
        "      {\n",
        "        \"utterances\": [\n",
        "          {\n",
        "            \"text_feat\": [float,...],   # dim_txt\n",
        "            \"audio_feat\": [float,...],  # dim_aud\n",
        "            \"vision_feat\": [float,...], # dim_vis\n",
        "            \"speaker\": \"A\",             # or int id\n",
        "            \"label\": 0                  # int class\n",
        "          },\n",
        "          ...\n",
        "        ]\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str, speaker2id: Optional[Dict[str, int]] = None):\n",
        "        super().__init__()\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.speaker2id = speaker2id if speaker2id is not None else {}\n",
        "        self._build_speaker_map()\n",
        "\n",
        "        # Infer dims\n",
        "        u0 = self.data[0][\"utterances\"][0]\n",
        "        self.dim_txt = len(u0[\"text_feat\"])\n",
        "        self.dim_aud = len(u0[\"audio_feat\"])\n",
        "        self.dim_vis = len(u0[\"vision_feat\"])\n",
        "\n",
        "        # Infer num classes\n",
        "        labels = []\n",
        "        for conv in self.data:\n",
        "            for u in conv[\"utterances\"]:\n",
        "                labels.append(int(u[\"label\"]))\n",
        "        self.num_classes = int(max(labels)) + 1\n",
        "\n",
        "    def _build_speaker_map(self):\n",
        "        for conv in self.data:\n",
        "            for u in conv[\"utterances\"]:\n",
        "                spk = u[\"speaker\"]\n",
        "                if isinstance(spk, int):\n",
        "                    key = str(spk)\n",
        "                else:\n",
        "                    key = str(spk)\n",
        "                if key not in self.speaker2id:\n",
        "                    self.speaker2id[key] = len(self.speaker2id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        conv = self.data[idx]\n",
        "        txt, aud, vis, spk, lab = [], [], [], [], []\n",
        "        for u in conv[\"utterances\"]:\n",
        "            txt.append(np.array(u[\"text_feat\"], dtype=np.float32))\n",
        "            aud.append(np.array(u[\"audio_feat\"], dtype=np.float32))\n",
        "            vis.append(np.array(u[\"vision_feat\"], dtype=np.float32))\n",
        "            sid = self.speaker2id[str(u[\"speaker\"])]\n",
        "            spk.append(sid)\n",
        "            lab.append(int(u[\"label\"]))\n",
        "        txt = np.stack(txt, axis=0)            # (T, Dt)\n",
        "        aud = np.stack(aud, axis=0)            # (T, Da)\n",
        "        vis = np.stack(vis, axis=0)            # (T, Dv)\n",
        "        spk = np.array(spk, dtype=np.int64)    # (T,)\n",
        "        lab = np.array(lab, dtype=np.int64)    # (T,)\n",
        "        return {\n",
        "            \"text\": txt,\n",
        "            \"audio\": aud,\n",
        "            \"vision\": vis,\n",
        "            \"speaker\": spk,\n",
        "            \"labels\": lab,\n",
        "            \"length\": txt.shape[0]\n",
        "        }\n",
        "\n",
        "\n",
        "class SyntheticMERC(Dataset):\n",
        "    \"\"\"\n",
        "    Generates synthetic multimodal conversations for quick testing.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_convs: int = 200,\n",
        "                 max_len: int = 12,\n",
        "                 min_len: int = 4,\n",
        "                 dim_txt: int = 300,\n",
        "                 dim_aud: int = 50,\n",
        "                 dim_vis: int = 64,\n",
        "                 num_speakers: int = 4,\n",
        "                 num_classes: int = 7,\n",
        "                 seed: int = 123):\n",
        "        super().__init__()\n",
        "        rng = np.random.RandomState(seed)\n",
        "        self.samples = []\n",
        "        for _ in range(num_convs):\n",
        "            T = rng.randint(min_len, max_len + 1)\n",
        "            txt = rng.normal(size=(T, dim_txt)).astype(np.float32)\n",
        "            aud = rng.normal(size=(T, dim_aud)).astype(np.float32)\n",
        "            vis = rng.normal(size=(T, dim_vis)).astype(np.float32)\n",
        "            spk = rng.randint(0, num_speakers, size=(T,), dtype=np.int64)\n",
        "            # Make labels weakly depend on a linear combo of modalities + speaker\n",
        "            logits = (txt[:, :8].sum(axis=1) + 0.5 * aud[:, :8].sum(axis=1) + 0.3 * vis[:, :8].sum(axis=1)\n",
        "                      + (spk.astype(np.float32) - num_speakers / 2.0))\n",
        "            # Bucketize into classes\n",
        "            q = np.quantile(logits, np.linspace(0, 1, num_classes + 1))\n",
        "            labels = np.digitize(logits, q[1:-1]).astype(np.int64)\n",
        "            self.samples.append({\n",
        "                \"text\": txt, \"audio\": aud, \"vision\": vis,\n",
        "                \"speaker\": spk, \"labels\": labels, \"length\": T\n",
        "            })\n",
        "        self.dim_txt, self.dim_aud, self.dim_vis = dim_txt, dim_aud, dim_vis\n",
        "        self.num_classes = num_classes\n",
        "        self.num_speakers = num_speakers\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int): return self.samples[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "    B = len(batch)\n",
        "    lengths = torch.tensor([b[\"length\"] for b in batch], dtype=torch.long)\n",
        "    T = int(lengths.max().item())\n",
        "    # infer dims\n",
        "    Dt = batch[0][\"text\"].shape[1]\n",
        "    Da = batch[0][\"audio\"].shape[1]\n",
        "    Dv = batch[0][\"vision\"].shape[1]\n",
        "\n",
        "    txt = torch.zeros(B, T, Dt, dtype=torch.float32)\n",
        "    aud = torch.zeros(B, T, Da, dtype=torch.float32)\n",
        "    vis = torch.zeros(B, T, Dv, dtype=torch.float32)\n",
        "    spk = torch.zeros(B, T, dtype=torch.long)\n",
        "    lab = torch.full((B, T), fill_value=-100, dtype=torch.long)  # ignore index for padding\n",
        "\n",
        "    for i, b in enumerate(batch):\n",
        "        t = b[\"length\"]\n",
        "        txt[i, :t] = torch.from_numpy(b[\"text\"]) if isinstance(b[\"text\"], np.ndarray) else torch.tensor(b[\"text\"])\n",
        "        aud[i, :t] = torch.from_numpy(b[\"audio\"]) if isinstance(b[\"audio\"], np.ndarray) else torch.tensor(b[\"audio\"])\n",
        "        vis[i, :t] = torch.from_numpy(b[\"vision\"]) if isinstance(b[\"vision\"], np.ndarray) else torch.tensor(b[\"vision\"])\n",
        "        spk[i, :t] = torch.from_numpy(b[\"speaker\"]) if isinstance(b[\"speaker\"], np.ndarray) else torch.tensor(b[\"speaker\"])\n",
        "        lab[i, :t] = torch.from_numpy(b[\"labels\"]) if isinstance(b[\"labels\"], np.ndarray) else torch.tensor(b[\"labels\"])\n",
        "\n",
        "    mask = lengths_to_mask(lengths, T)  # (B, T)\n",
        "    return {\n",
        "        \"text\": txt, \"audio\": aud, \"vision\": vis,\n",
        "        \"speaker\": spk, \"labels\": lab, \"lengths\": lengths, \"mask\": mask\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "\n",
        "class MLPEncoder(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int, p_drop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "    \"\"\"Early fusion with learnable gates per modality.\"\"\"\n",
        "    def __init__(self, d_txt, d_aud, d_vis, d_out):\n",
        "        super().__init__()\n",
        "        self.txt_enc = MLPEncoder(d_txt, d_out)\n",
        "        self.aud_enc = MLPEncoder(d_aud, d_out)\n",
        "        self.vis_enc = MLPEncoder(d_vis, d_out)\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(d_out * 3, d_out * 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(d_out * 3, 3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.proj = nn.Linear(d_out * 3, d_out)\n",
        "\n",
        "    def forward(self, txt, aud, vis):\n",
        "        # (B, T, D) -> encode\n",
        "        h_t = self.txt_enc(txt)\n",
        "        h_a = self.aud_enc(aud)\n",
        "        h_v = self.vis_enc(vis)\n",
        "        H = torch.cat([h_t, h_a, h_v], dim=-1)\n",
        "        g = self.gate(H)  # (B, T, 3)\n",
        "        g_t, g_a, g_v = g[..., 0:1], g[..., 1:2], g[..., 2:3]\n",
        "        fused = torch.cat([g_t * h_t, g_a * h_a, g_v * h_v], dim=-1)\n",
        "        return self.proj(fused)  # (B, T, d_out)\n",
        "\n",
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    \"\"\"Bahdanau-style attention over temporal context.\"\"\"\n",
        "    def __init__(self, d_ctx: int, d_attn: int):\n",
        "        super().__init__()\n",
        "        self.fc_h = nn.Linear(d_ctx, d_attn, bias=False)\n",
        "        self.fc_q = nn.Linear(d_ctx, d_attn, bias=False)\n",
        "        self.v = nn.Linear(d_attn, 1, bias=False)\n",
        "\n",
        "    def forward(self, H: torch.Tensor, q: torch.Tensor, mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        H: (B, T, d_ctx)  context sequence\n",
        "        q: (B, T, d_ctx)  per-timestep query (e.g., same as H or projected)\n",
        "        mask: (B, T)      valid positions\n",
        "        returns: attended context per timestep: (B, T, d_ctx)\n",
        "        \"\"\"\n",
        "        Wh = self.fc_h(H)                # (B, T, d_attn)\n",
        "        Wq = self.fc_q(q)                # (B, T, d_attn)\n",
        "        scores = self.v(torch.tanh(Wh.unsqueeze(1) + Wq.unsqueeze(2))).squeeze(-1)  # (B, T, T)\n",
        "        # mask invalid keys\n",
        "        key_mask = mask.unsqueeze(1).expand_as(scores)  # (B, T, T)\n",
        "        scores = scores.masked_fill(~key_mask, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)            # (B, T, T)\n",
        "        ctx = torch.bmm(attn, H)                        # (B, T, d_ctx)\n",
        "        return ctx\n",
        "\n",
        "\n",
        "class MERCLSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim_txt: int,\n",
        "                 dim_aud: int,\n",
        "                 dim_vis: int,\n",
        "                 num_speakers: int,\n",
        "                 num_classes: int,\n",
        "                 d_model: int = 256,\n",
        "                 d_attn: int = 128,\n",
        "                 d_spk: int = 32,\n",
        "                 lstm_layers: int = 1,\n",
        "                 dropout: float = 0.2,\n",
        "                 bidirectional: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fusion = GatedFusion(dim_txt, dim_aud, dim_vis, d_model)\n",
        "\n",
        "        self.spk_emb = nn.Embedding(num_speakers, d_spk)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model + d_spk,\n",
        "            hidden_size=d_model // 2 if bidirectional else d_model,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0.0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        d_ctx = (d_model // 2) * 2 if bidirectional else d_model\n",
        "\n",
        "        self.attn = AdditiveAttention(d_ctx, d_attn)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_ctx * 2, d_ctx),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ctx, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text, audio, vision, speaker, mask):\n",
        "        \"\"\"\n",
        "        text/audio/vision: (B, T, D*)\n",
        "        speaker: (B, T) long\n",
        "        mask: (B, T) bool\n",
        "        returns logits: (B, T, C)\n",
        "        \"\"\"\n",
        "        fused = self.fusion(text, audio, vision)  # (B, T, d_model)\n",
        "        spk = self.spk_emb(speaker)               # (B, T, d_spk)\n",
        "        x = torch.cat([fused, spk], dim=-1)       # (B, T, d_model+d_spk)\n",
        "\n",
        "        lengths = mask.sum(dim=1).long().cpu()\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.lstm(packed)\n",
        "        H, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T, d_ctx)\n",
        "\n",
        "        # Attention with H as both keys/values and query\n",
        "        ctx = self.attn(H, H, mask)               # (B, T, d_ctx)\n",
        "        out = torch.cat([H, ctx], dim=-1)         # (B, T, 2*d_ctx)\n",
        "        logits = self.classifier(out)             # (B, T, C)\n",
        "\n",
        "        # mask out padding logits for safety (not strictly necessary if using ignore_index)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training / Evaluation\n",
        "# ---------------------------\n",
        "\n",
        "def masked_cross_entropy(logits: torch.Tensor, targets: torch.Tensor, ignore_index: int = -100):\n",
        "    \"\"\"\n",
        "    logits: (B, T, C), targets: (B, T)\n",
        "    \"\"\"\n",
        "    B, T, C = logits.shape\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
        "    return loss_fn(logits.view(B * T, C), targets.view(B * T))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    all_preds, all_golds = [], []\n",
        "    for batch in loader:\n",
        "        text = batch[\"text\"].to(device)\n",
        "        audio = batch[\"audio\"].to(device)\n",
        "        vision = batch[\"vision\"].to(device)\n",
        "        speaker = batch[\"speaker\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "\n",
        "        logits = model(text, audio, vision, speaker, mask)  # (B, T, C)\n",
        "        preds = logits.argmax(dim=-1)                       # (B, T)\n",
        "\n",
        "        # Flatten valid positions\n",
        "        valid = mask.view(-1).cpu().numpy().astype(bool)\n",
        "        gold = labels.view(-1).cpu().numpy()\n",
        "        prd = preds.view(-1).cpu().numpy()\n",
        "        gold = gold[valid]\n",
        "        prd = prd[valid]\n",
        "\n",
        "        all_preds.append(prd)\n",
        "        all_golds.append(gold)\n",
        "\n",
        "    y_true = np.concatenate(all_golds, axis=0)\n",
        "    y_pred = np.concatenate(all_preds, axis=0)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    return {\"accuracy\": acc, \"f1_macro\": f1m}\n",
        "\n",
        "\n",
        "def train_loop(model: nn.Module,\n",
        "               train_loader: DataLoader,\n",
        "               valid_loader: Optional[DataLoader],\n",
        "               device: torch.device,\n",
        "               epochs: int = 20,\n",
        "               lr: float = 1e-3,\n",
        "               weight_decay: float = 1e-4,\n",
        "               grad_clip: float = 1.0,\n",
        "               ckpt_path: Optional[str] = None):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    best_f1 = -1.0\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        n_steps = 0\n",
        "        for batch in train_loader:\n",
        "            text = batch[\"text\"].to(device)\n",
        "            audio = batch[\"audio\"].to(device)\n",
        "            vision = batch[\"vision\"].to(device)\n",
        "            speaker = batch[\"speaker\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            mask = batch[\"mask\"].to(device)\n",
        "\n",
        "            logits = model(text, audio, vision, speaker, mask)\n",
        "            loss = masked_cross_entropy(logits, labels)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            if grad_clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            opt.step()\n",
        "\n",
        "            running += loss.item()\n",
        "            n_steps += 1\n",
        "\n",
        "        msg = f\"[Epoch {ep:02d}] Train Loss: {running / max(n_steps,1):.4f}\"\n",
        "\n",
        "        if valid_loader is not None:\n",
        "            metrics = evaluate(model, valid_loader, device)\n",
        "            msg += f\" | Val Acc: {metrics['accuracy']:.4f} | Val F1m: {metrics['f1_macro']:.4f}\"\n",
        "            # Save best\n",
        "            if metrics[\"f1_macro\"] > best_f1 and ckpt_path:\n",
        "                best_f1 = metrics[\"f1_macro\"]\n",
        "                torch.save({\n",
        "                    \"model_state\": model.state_dict(),\n",
        "                    \"config\": {\n",
        "                        \"num_classes\": model.classifier[-1].out_features\n",
        "                    }\n",
        "                }, ckpt_path)\n",
        "                msg += \"  [*saved*]\"\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Main / CLI\n",
        "# ---------------------------\n",
        "\n",
        "def build_loaders(args) -> Tuple[DataLoader, Optional[DataLoader], Dict[str, int], Dict[str, int]]:\n",
        "    if args.data_json and os.path.isfile(args.data_json):\n",
        "        dataset = JSONMERC(args.data_json)\n",
        "        # Split 80/20\n",
        "        idxs = list(range(len(dataset)))\n",
        "        random.shuffle(idxs)\n",
        "        cut = int(0.8 * len(dataset))\n",
        "        train_idxs, val_idxs = idxs[:cut], idxs[cut:]\n",
        "        train_data = torch.utils.data.Subset(dataset, train_idxs)\n",
        "        val_data = torch.utils.data.Subset(dataset, val_idxs)\n",
        "        num_speakers = len(dataset.speaker2id)\n",
        "        num_classes = dataset.num_classes\n",
        "        dim_txt, dim_aud, dim_vis = dataset.dim_txt, dataset.dim_aud, dataset.dim_vis\n",
        "    else:\n",
        "        # synthetic\n",
        "        syn = SyntheticMERC(\n",
        "            num_convs=args.syn_num_convs,\n",
        "            max_len=args.syn_max_len,\n",
        "            min_len=args.syn_min_len,\n",
        "            dim_txt=args.syn_dim_txt,\n",
        "            dim_aud=args.syn_dim_aud,\n",
        "            dim_vis=args.syn_dim_vis,\n",
        "            num_speakers=args.syn_num_speakers,\n",
        "            num_classes=args.syn_num_classes,\n",
        "            seed=args.seed\n",
        "        )\n",
        "        # Split 80/20\n",
        "        idxs = list(range(len(syn)))\n",
        "        random.shuffle(idxs)\n",
        "        cut = int(0.8 * len(syn))\n",
        "        train_data = torch.utils.data.Subset(syn, idxs[:cut])\n",
        "        val_data = torch.utils.data.Subset(syn, idxs[cut:])\n",
        "        num_speakers = syn.num_speakers\n",
        "        num_classes = syn.num_classes\n",
        "        dim_txt, dim_aud, dim_vis = syn.dim_txt, syn.dim_aud, syn.dim_vis\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data, batch_size=args.batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_data, batch_size=args.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    dims = {\"dim_txt\": dim_txt, \"dim_aud\": dim_aud, \"dim_vis\": dim_vis}\n",
        "    meta = {\"num_speakers\": num_speakers, \"num_classes\": num_classes}\n",
        "    return train_loader, val_loader, dims, meta\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Multimodal LSTM for Emotion Recognition in Conversation\")\n",
        "    parser.add_argument(\"--data_json\", type=str, default=\"\", help=\"Path to JSON dataset. If empty, uses synthetic.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
        "    parser.add_argument(\"--grad_clip\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--seed\", type=int, default=1337)\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    parser.add_argument(\"--ckpt\", type=str, default=\"best_merc_lstm.pt\")\n",
        "\n",
        "    # Synthetic data settings (used if --data_json not provided)\n",
        "    parser.add_argument(\"--syn_num_convs\", type=int, default=200)\n",
        "    parser.add_argument(\"--syn_min_len\", type=int, default=4)\n",
        "    parser.add_argument(\"--syn_max_len\", type=int, default=12)\n",
        "    parser.add_argument(\"--syn_dim_txt\", type=int, default=300)\n",
        "    parser.add_argument(\"--syn_dim_aud\", type=int, default=50)\n",
        "    parser.add_argument(\"--syn_dim_vis\", type=int, default=64)\n",
        "    parser.add_argument(\"--syn_num_speakers\", type=int, default=4)\n",
        "    parser.add_argument(\"--syn_num_classes\", type=int, default=7)\n",
        "\n",
        "    # Model hyperparams\n",
        "    parser.add_argument(\"--d_model\", type=int, default=256)\n",
        "    parser.add_argument(\"--d_attn\", type=int, default=128)\n",
        "    parser.add_argument(\"--d_spk\", type=int, default=32)\n",
        "    parser.add_argument(\"--lstm_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--bidirectional\", action=\"store_true\", help=\"Use BiLSTM\")\n",
        "    parser.add_argument(\"--no-bidirectional\", dest=\"bidirectional\", action=\"store_false\")\n",
        "    parser.set_defaults(bidirectional=True)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    train_loader, val_loader, dims, meta = build_loaders(args)\n",
        "    device = torch.device(args.device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = MERCLSTM(\n",
        "        dim_txt=dims[\"dim_txt\"], dim_aud=dims[\"dim_aud\"], dim_vis=dims[\"dim_vis\"],\n",
        "        num_speakers=meta[\"num_speakers\"], num_classes=meta[\"num_classes\"],\n",
        "        d_model=args.d_model, d_attn=args.d_attn, d_spk=args.d_spk,\n",
        "        lstm_layers=args.lstm_layers, dropout=args.dropout,\n",
        "        bidirectional=args.bidirectional\n",
        "    ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    train_loop(model, train_loader, val_loader, device,\n",
        "               epochs=args.epochs, lr=args.lr, weight_decay=args.weight_decay,\n",
        "               grad_clip=args.grad_clip, ckpt_path=args.ckpt)\n",
        "\n",
        "    # Final evaluation on validation set with best checkpoint if available\n",
        "    if os.path.isfile(args.ckpt):\n",
        "        print(f\"Loading best checkpoint: {args.ckpt}\")\n",
        "        ckpt = torch.load(args.ckpt, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model_state\"])\n",
        "    metrics = evaluate(model, val_loader, device)\n",
        "    print(f\"[Final] Val Acc: {metrics['accuracy']:.4f} | Val F1m: {metrics['f1_macro']:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}